# Author Nigel Slack
# Date 11/Mar/2022

# Held in Github : https://github.com/NigelSlack/Axivity-Sensor

helpText = """
The Axivity AX3 accelerometer is a wearable sensor that measures acceleration values in 3 orthogonal directions - x, y and z.

This code has three basic functions : 
  Label output csv records from the sensor by activity types performed by a user 
  Build machine learning models using the labelled data for automatic recognition of activity type 
  Run these models against unlabelled csv files from the sensor to automatically label them

Requirements : Python, plus required libraries (see 'loadLibraries' function) which can be obtained using 'pip install', eg pip install csv
Note : 'pip install pyyaml h5py' is needed for Neural Network processing.
On running the code, if any required libraries are not present for the selected processing, a message is output for each one missing, and the run terminates.

To run :
  python axivityProcess.py

Configuration of the sensor is performed using the 'Open Movement' (OM) software package that accompanies it, via a USB or wireless connection to a computer.
Recording of data is started using OM, activities are undertaken by a user, then recording is stopped in OM.
A compressed data file of type 'cwa' is generated by the sensor, with each record containing a timestamp, acceleration readings in the x, y and z directions,
and temperature readings.
The x, y and z directions will depend on the orientation of the sensor during the time it is recording.
OM has a facility for exporting the resultant 'cwa' files to other file types, such as csv files.
 
This code processes exported files (from OM) of type 'raw csv', with datetime format Y-M-D h:m:s.f, and Accelerometer Units Gravity(g). 
Temperature readings are not processed as they are likely to be of little value for the purpose of predicting activity type.

The user selects from one of nine functions (by specifying the number of the function, as displayed) -
1) Label csv file from sensor                                        
2) Combine labelled csv files                                        
3) Create kMeans model from labelled file                            
4) Create Machine Learning models from labelled file                 
5) Create Neural Network model from labelled file                    
6) Run kMeans model against unlabelled csv file from sensor          
7) Run Machine Learning model against unlabelled csv file from sensor
8) Run Neural Network model against unlabelled csv file from sensor  
9) Help                                                              

For all functions except 'Help', processing starts with the user selecting an input file (by clicking it in a separate Explorer window opened by the code)

When manually labelling a file from the sensor, the output consists of 2 csv files:
  The first has a subject id (eg the user's initials), and an abbreviation of the sensor location (eg 'LW' for Left Wrist) appended to the original file name
  The second has '_sum' appended to this first file name. 
When creating a machine learning model it is saved to a folder specified by the user.
When running a model against an unlabelled file an output file can be created containing predicted activities, again in a location 
selected by the user. This will have the same format as the first file above.  

For 'Label csv file from sensor' :
The first output file has format :
  Timestamp  x-val  y-val  z-val  rms-val  activity    
where 'rms-val' is the root mean square of the x, y and z values 
eg 22/02/2022 18:07:16	0.203125	0.28125	-0.9375	0.999633722	Walk Upstairs
 
Note : If the csv files are viewed using Excel, the timestamp values may appear as '########' in the table cells.
       To view them as timestamps :  
          right-click the first cell
          select 'format cells'
          'custom' with 'type' dd/mm/yyyy hh:mm , in the box immediately below 'Type' append ':ss' (for seconds)
          'ok'
(if other datetime formats are used the code will fail to run)

The 'sum' file gives a summary of the means and standard deviations for each block of activity for each of the x, y, z and rms values and how long the activity
lasted in seconds, eg
Subject Activity        Location   Time             Duration(s)	x mean x std y mean y std z mean z std rms mean	rms std
NS      Walk Upstairs   Left Wrist 22/02/2022 18:08 66          -0.4   0.32  0.4    0.74  0.03   0.33  1        0.35

Several prompts are displayed for user input :
The number of readings per second in the file is determined by the code, and the user is prompted to specify how many they wish to retain.
 The sensor can record between 12.5 and 500 readings per second (Hz), but the OM software warns that it may not process signals correctly if the rate
 is set below 50Hz. It is probably best therefore to use this as a minimum frequency value when configuring the sensor. However, this number of readings
 may not be required to determine activity type, eg 10 or 20 readings per second may be sufficient to distinguish between different activities.
A plot is then displayed of the input data, and the user left clicks all the transitions between activities for the time period of interest. When finished they right click.
If the magnifier tool is used, selected areas to magnify should be defined with y-values well above/below the y-values used to set activity transitions.
These clicks will be discarded afterwards, and not mistaken as transition points.
A list of activities is shown, and for each time period the user specifies the associated activity.
Periods for which the user selects 'Other' as the activity type will be discarded (although the stats for them will be included in the ..._sum.csv file).
If the user selects any 'Other' activities, the timestamps of subsequent periods are adjusted to run immediately consecutive to the activity preceding the period for 'Other'.
The timestamps will no longer be the actual timestamps, but this does not affect using the output files for training models, and using the trained models for predicting activities
for unlabelled files.
The next prompt is for Subject id - eg the initials of the person using the sensor for the selected csv file. 
The user is then asked for the sensor location during the activities, eg on the Left Wrist. It is assumed the same location applies to all activities stored
in the one file. A list of activities is defined at the start of the code, which can easily be edited.
The output file is then written, with the root mean square value of the x, y and z values, and the associated activity being added to each input record.
Finally, the user can display a plot of the resultant file.

'Combine labelled csv files' produces a single labelled file from multiple input files. This may be useful for combining different files that have been 
created by a user containing different activities. A model can then be trained against the combined file containing all the activity types. 
eg a user might perform 'walk' and 'sit' activities and save them to one file, then perform 'run' and 'drive' and save to a second file, then save 'sleep' to 
a third file. All these can be amalgamated into one output file.
When training models to recognise activity types the values in timestamp are not used (but the created records should stay in the same chronological order for
reasons of pattern recognition).
The first input file is simply copied to the output file and the timestamp of the last record in the file is saved.
One second is added to this saved timestamp, and for the next input file all records are appended to the end of the output file, but with the
timestamps replaced - starting with the (timestamp + 1 second) as the replacement value, and incrementing this by one second each time the 'second' value
changes in the input file. For each subsequent file the modified timestamp is used, adding one second for the start of each file. 
The resultant output file then contains a set of records with all the input activities in a contiguous block (time-wise).

For options 3, 4 and 5 (creating models), after selecting the (labelled) input file, the user selects a parent folder to create the folder in that will contain
the model (from a separate Explorer window, opened by the code), and then they specify the name for the sub-folder for holding the model.
The number of records in the selected input data is displayed, and the user is asked how many they wish to retain as a random sample from this data set (between 1000
and the total number of records). The more records that are used for training the models, the longer the process takes. The user may wish to experiment with different 
values here.
They are then asked if they wish to scale the data. Scaling may help prevent large values for one variable from swamping those for the others, but it may also result in
worse prediction outcomes if the uniqueness of input patterns is made less distinct. Again, experimentation is required. 
The model is then created in the specified sub-folder (for option 4, 5 models are created).

For options 6, 7 and 8 (using previously created models to process unlabelled files from the sensor), after selecting the (unlabelled) 
input file, the user selects the directory containing the model. They are then shown the activities catered for by the model (the ones
in the file the model was created from) and asked if they wish to continue. If they respond 'N' processing stops. Otherwise a plot is
displayed of the input data and the user selects the start and end times of the data of interest using mouse left clicks
(each input file will have some data at the start and end that should be discarded - when the user is putting the sensor on/taking it off - 
if included this may result in poorer prediction accuracy by the model). 
As for options 3, 4 and 5 the user can restrict the number of data records processed (to reduce run time) and scale/not scale the data - experiment.
A plot of the input data, plus rms and predicted activities is then displayed. Finally the user can produce a labelled file (using the model 
predictions) - they are prompted for the file name to create.

Means of classifying data - most common techniques - KNN, kmeans and sub-clustering. From review paper -
https://www.researchgate.net/publication/341303680_Unsupervised_Human_Activity_Recognition_Using_the_Clustering_Approach_A_Review
"""

#---------------------------------------------------------------------------------------------------------------------------------------------
# Get required libraries. Check each one loads ok. If not, tell the user to pip install. 
# 'optNum' and 'libNums' are used to determine which libraries are required based on the requested processing - so that unnecessary libraries
# are not loaded.
#---------------------------------------------------------------------------------------------------------------------------------------------
def loadLibrary(lib):
  global allOk, optNum, libNums
  if not optNum in libNums:
    return
  try:
    exec(lib)
# Make the libraries available globally  
    globals().update(locals())
  except ModuleNotFoundError:
    if allOk:
      print(" ")
      allOk = False
    print(lib,"- import failed. Please pip install and retry") 
    pass

def loadLibraries():
  global allOk, libNums
  print("Please wait, loading libraries...")
  allOk = True

# 'libNums' is set to a list containing the numeric value of all purposes for which the code is run, for those libraries required by all purposes.  
  libNums = list(range(0,len(purposeList)))
  loadLibrary("from tkinter import filedialog as fd")
  loadLibrary("from csv import reader, writer")
  loadLibrary("import os")
  loadLibrary("import csv")
  loadLibrary("import datetime")
  loadLibrary("import pandas as pd")
  loadLibrary("import numpy as np")
  loadLibrary("import matplotlib.ticker")
# AutoMinorLocator is used to change the number of minor x-axis 'ticks' (and grid lines) in the data plots
# Here we're setting it to 15 minor ticks between each major tick
  class MyLocator(matplotlib.ticker.AutoMinorLocator):
    def __init__(self, n=15):
        super().__init__(n=n)
  matplotlib.ticker.AutoMinorLocator = MyLocator  
  loadLibrary("import matplotlib.pyplot as plt")
  
  libNums = [0]
  loadLibrary("import statistics")
  
  libNums = [0,4]  
  loadLibrary("import random")

  libNums = [2,3,4,5,6,7]
  loadLibrary("from scipy import stats")
  loadLibrary("from sklearn.preprocessing import RobustScaler")
    
  libNums = [2]
  loadLibrary("from sklearn.cluster import KMeans")  
  loadLibrary("from sklearn.preprocessing import MinMaxScaler")
  loadLibrary("from itertools import permutations") 
    
  libNums = [3] 
  loadLibrary("from sklearn.neighbors import KNeighborsClassifier")
  loadLibrary("from sklearn.tree import DecisionTreeClassifier")
  loadLibrary("from sklearn.svm import SVC")
  loadLibrary("from sklearn.naive_bayes import GaussianNB")
  loadLibrary("from sklearn.ensemble import BaggingClassifier")
  loadLibrary("from sklearn.ensemble import RandomForestClassifier")  
  loadLibrary("from sklearn.metrics import accuracy_score")  
  loadLibrary("from sklearn.model_selection import train_test_split")  
  loadLibrary("from sklearn.model_selection import GridSearchCV")

  libNums = [4] 
  loadLibrary("from sklearn.preprocessing import OneHotEncoder")
  loadLibrary("import math")
  loadLibrary("from statistics import mean")
  
  libNums = [1]
  loadLibrary("import shutil")
  loadLibrary("from datetime import datetime")
  loadLibrary("from datetime import timedelta")

  libNums = [2,3,5,6]
  loadLibrary("import pickle")

  libNums = [4,7]
# Disable warning messages output from 'Tensorflow', eg if the computer running the code doesn't have any GPUs
# (which Tensorflow can use to improve performance) 
  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
  loadLibrary("import tensorflow as tf")

# If a library didn't load, stop.  
  if not allOk:
    print("\nTerminating .. ")
    exit()
  
#-----------------------------------------------------------------------------------------------------------------------------------
# Set up 3 lists - 
#  1. The functions the code performs 
#  2. The activities catered for
#  3. The location where the sensor is worn
#
# If 'purposeList' changes the code will need changing in the main processing, at the bottom of the code, where an 'if' command is 
# used to determine the processing required. 
# eg this 'if' contains 'elif "Create Neural" in purpose:', so if the text here is changed the 'if' command will need changing too.
#
# The contents of the other two lists don't affect the running of the code, but the actual text in 'activityList' is written into
# the labelled csv file as the activity.
#-----------------------------------------------------------------------------------------------------------------------------------

purposeList =  """
               Label csv file from sensor
               Combine labelled csv files
               Create kMeans model from labelled file
               Create Machine Learning models from labelled file
               Create Neural Network model from labelled file               
               Run kMeans model against unlabelled csv file from sensor
               Run Machine Learning model against unlabelled csv file from sensor
               Run Neural Network model against unlabelled csv file from sensor
               Help
               """ 


activityList = """
               Walk Up/down stairs
               Walk on level
               Drive
               Light house activity
               Heavy house activity 
               Run on level
               Sit
               Clap
               Jump
               Other
               """ 

locationList = """
               Left Wrist (LW)
               Right Wrist (RW)
               Left Ankle (LA)
               Right Ankle (RA)
               Waist (WA)
               """ 

#---------------------------
# Set up all the functions |
#---------------------------

#-------------------------------------
# Check if a value is floating point |
#-------------------------------------
def is_float(element):
    try:
        float(element)
        return True
    except ValueError:
        return False

#---------------------------------
# Check if a value is an integer |
#---------------------------------
def is_int(element):
    try:
        int(element)
        return True
    except ValueError:
        return False

#------------------------------------------------------------------
# Ask the user if they wish to continue processing. If not, stop. |  
#------------------------------------------------------------------
def checkIfContinue():
  ok = ""
  print(" ")
  while ok != "Y" and ok != "N":
    ok = input("Continue (Y/N)? - ").upper()
    if ok == "N":
      print("\nTerminating")
      exit()        

#--------------------------------------------------------------------------------------------------------------
# Create a list of options (activity/sensor location) to present to the user, with each option being numbered | 
#--------------------------------------------------------------------------------------------------------------
def setupList(inList):
    inTemp = inList.splitlines()
    option = []
    for line in inTemp:
        line = line.strip()
        if len(line) > 0: 
          option.append(line)
    return(option)

#--------------------------------------------------------------------------------------
# Get the user to select an option (created in 'setupList') by specifying it's number |
#--------------------------------------------------------------------------------------
def letUserPick(options):
    global optNum
    done = False
    while not done:
      for idx, element in enumerate(options):
        print("{}) {}".format(idx+1,element))
      i = input("\nEnter number: ")
      try:
        if 0 < int(i) <= len(options):
            done = True
            print(" ")
            optNum = int(i)-1
            return options[int(i)-1]
      except:
        pass

#------------------------------------------------------------------------------------------------------------------------------------
# For each two second interval set the activity to the most common one currently found within that interval.
# A prediction from eg a Machine Learning algorithm might for example predict a quarter second of clapping surrounded by
# 5 seconds of walking upstairs before and after the clapping. In such a case it is likely the clapping has been mis-characterised. 
# We assume activities are performed for at least 2 seconds, and by labelling in 2 second blocks helps eliminate single, or small
# numbers of records being labelled with activities that are probably incorrect - the most common label is most likely to be correct 
# within an interval.
#------------------------------------------------------------------------------------------------------------------------------------
def smoothActivities(predictedAct):
  ilen = len(predictedAct)
  i = 0
# 'freq' contains the number of data recods per second. 
  numrecs = freq * 30
  ys = []
  while i < ilen:
    labels = predictedAct[i: i + numrecs]
    i+=numrecs
    if i >= ilen:
      numrecs = ilen-(i-numrecs)
    ys.append(numrecs*[stats.mode(labels)[0][0]])
# stats.mode returns the most common value (of predicted activities)in the array (subset of x,y,z values and the associated
# activity)
  activities=[x for sublist in ys for x in sublist]  
  return activities

#---------------------------------------------------------------------------------------------------------------------
# Find how many entries the input file has per second - find the first change of second, then count how many entries
# there are to the next change. This is used to tell the user what the frequency was in case they wish to reduce the
# number of entries in the labelled file. It is also used to 'smooth' predicted activities over two second periods.
# Also make a note of the time of the first entry in the file for use later when stripping records from the start of
# the file, and setting the start time of the first activity, and the format of the timestamp.
# Return the number of columns in a record ('len(row)'). When the user selects a data file for processing the number
# of columns is used to check they've selected a (possibly) valid data file for the requested processing.
#---------------------------------------------------------------------------------------------------------------------
def find_frequency(filename):
  global startTime
  fSec = True
  newSec = ""
  with open(filename, 'r') as csvFile:
    reader = csv.reader(csvFile)
    ix = 0
    for row in reader:        
        if fSec:
          fSec = False
# Get the timestamp for the first record. Make sure the year value is first - if the time format is dd-mm-yyyy switch 
# the dd and yyyy round (so we know what format we're dealing with later on)          
          if sy:
            startTime = row[0][0:4] + bchar + row[0][5:7] + bchar + row[0][8:19]                    
          else:
            startTime = row[0][6:10] + bchar + row[0][3:5] + bchar + row[0][0:2] + row[0][10:19]  
# Record the 'seconds' value            
          fSecond = row[0][0:19]
        if row[0][0:19] != fSecond:
# Look for the 'second' value changing. Stop when we get to a new 'second' value         
          if ix == 0:
            newSec = row[0][0:19]
          else:
            if newSec != row[0][0:19]:
              freq = ix-1
              break
          ix+=1
  csvFile.close()
  return (freq,len(row))

#---------------------------------------------------------------------------------------------------------------------------------
# Ask the user to select a folder. A separate Explorer window opens (in 'askdirectory') from which the user can select a folder.
# If the user selects a folder other than the current one, switch to it.  
#---------------------------------------------------------------------------------------------------------------------------------
def selectFolder():
  fld = ""
  while fld != "C" and fld != "S":
    fld = input("\nUse current folder (C) or select folder (S) (C/S) - ").upper()   
    if fld == "S":
      currentFolder = os.getcwd()
      folder = fd.askdirectory()
      os.chdir(folder)

#----------------------------------------  
# Insert a title on a (matplotlib) plot |
#----------------------------------------  
def tellme(s):
    plt.title(s, fontsize=12)
    plt.draw()

#--------------------------------------------------------------------------------------------------
# Scale data so that large values for one variable don't have an undue influence on an algorithm, 
# resulting in the values of other variables being under-represented in the processing.
#--------------------------------------------------------------------------------------------------    
def scaleInputData(df):
  ok = ""
  while ok != "Y" and ok != "N":
    ok = input("\nScale input data? Y/N - ").upper()
    if ok == "N":
      return df
  print("\nScaling data ...")    
  scale_columns = ['x_axis', 'y_axis', 'z_axis']
  scaler = RobustScaler()
  scaler = scaler.fit(df[scale_columns])
  df.loc[:, scale_columns] = scaler.transform(df[scale_columns].to_numpy())
  return df    

#-----------------------------------------------------------------------------------------------------------------------------------
# When a model is saved, the list of unique activities in the data file used to build the model are saved into a file called
# 'activities.csv'. When this model is run to categorize unlabelled data the user is told which activities were contained in the 
# data the model was trained on. It would be inappropriate to run a model against an unlabelled data file created by a user
# performing activities for which the model has not been trained.
# For a Neural Network model the 'activities' file also contains two records showing the length of the blocks that the data was 
# grouped into, and how much overlap there was between adjacent blocks (in the first 2 records - TIME_STEPS and STEP respectively).
#-----------------------------------------------------------------------------------------------------------------------------------
def showModelActivities(actFile, stepInd):
  aList, TIME_STEPS, STEP = [] , 0 , 0
  print("\nThe activities analysed by this model were :")
  with open(actFile, 'r') as csvFile:
    reader = csv.reader(csvFile)
    ind = 0
    for row in reader: 
      for r in row:
        if (ind == 0) and stepInd:
          TIME_STEPS = int(r)
        elif (ind == 1) and stepInd:
          STEP = int(r)
        else:        
          print(" ",r)   
          aList.append(r)  
        ind+=1          
  csvFile.close()

# After showing the user the activities the model was trained on, ask them if they wish to continue processing.  
  checkIfContinue()

  return aList, TIME_STEPS, STEP

#------------------------------------------------------------------------------------------------------------------------------------------  
# Enable the user to manually label a csv file from the sensor, or to select the start and end times of the data of interest.
# A plot is displayed of the input data in a separate window to help the user label the entries with associated activities 
# (by knowing beforehand what activities were performed and for roughly how long), or to select the start time and the stop time
# of the data to be processed.
# For manual labelling - 
#   The user left clicks on the plot for each transition between activities. After all transitions have been clicked the user right clicks.
#   They are then presented with a list of time periods, and for each one they specify what the activity was. 
# For selecting the start and stop times - 
#   The user left clicks the start time, then left clicks the end time, then right clicks.
#------------------------------------------------------------------------------------------------------------------------------------------  
def plot_raw_data(filename,trimTimes,tellText):
  global trimStart, trimEnd, uniqueAct, newVals, startTime, stopTime
  plt.rcParams["figure.figsize"] = [14.50, 6.50]
  plt.rcParams["figure.autolayout"] = True
# The input data (from the Axivity sensor) has 4 values - time and x,y,z acceleration values  
  headers = ['Time', 'x_axis', 'y_axis', 'z_axis']
  df = pd.read_csv(filename, names=headers) 
  df['Time'] = pd.to_datetime(df['Time'])
# Round the 'time' part to the nearest second  
  df['Time'] = df['Time'].dt.round('1s')
  df.set_index('Time').plot()
  plt.xticks(rotation=90)
  plt.minorticks_on()
# Put gridlines on the plot to make it easier for the user to determine when activities change  
  plt.grid(b=True, which='major', axis='x', color='0.65', linestyle='-', lw = 1)  
  plt.grid(b=True, which='minor', axis='x', color='0.65', linestyle='--', lw = 1)  
  axes = plt.gca()
  plt_y_min, plt_y_max = axes.get_ylim()

# 'plt.ginput' gets user inputs from the plot (mouse clicks - left click adds a value to the output, backspace/delete removes a value,
# and right click terminates inputs).
# If 'trimTimes' is True the user is selecting start and end times of the data of interest - there must be exactly two left clicks.
  tellme(tellText)
  xp = ""
  if trimTimes:
    while len(xp) != 2:
      xp = plt.ginput(-1,timeout=-1,mouse_add=1, mouse_pop=2, mouse_stop=3)
      if len(xp) != 2:
        print("\nYou must left click on two points - the start and end times of the data of interest. Then right click.")
  else:  
# If 'trimTimes' is False the user is manually labelling data. There must be a minium of two left clicks, but often there should
# be more (the user will have performed more than one activity during the time period covered by the data file). If they've only
# done two left clicks check with them that's what they meant to do.  
    while len(xp) < 2:
      xp = plt.ginput(-1,timeout=-1,mouse_add=1, mouse_pop=2, mouse_stop=3)
      if len(xp) == 2:
        print("\nAre you sure you've selected all the activity transitions?")
        ok = ""
        while ok != "Y" and ok != "N":
          ok = input("Y/N - ").upper()
          if ok == "N":
            xp = []
      
# If the user used the magnifier tool on the plot there will be clicks (used to select areas of the plot to magnify) that were not for activity selection.
# The user should have clicked on y-values well above/below the values clicked when selecting activities when they used the magnifier. We'll discard 
# clicks with y-values above/below values specified by the user.
    print("\nWas the magnifier tool used?")
    ok, yMax, yMin = "", 0, 0
    while ok != "Y" and ok != "N":
      ok = input("Y/N - ").upper()
      if ok == "Y":
        print("\n Enter the y-values above/below which clicks should be ignored (because they were used for magnifying)")
        while yMax == 0:
          txt = "\nEnter y-value above which clicks are ignored (below " + str(plt_y_max) + ") : "
          yMax = input(txt)
          if not (is_int(yMax) or is_float(yMax)):
            yMax = 0
          else:
            if float(yMax) > plt_y_max:
              yMax = 0            
        while yMin == 0:
          txt = "\nEnter y-value below which clicks are ignored (above " + str(plt_y_min) + ") : "
          yMin = input(txt)
          if not (is_int(yMin) or is_float(yMin)):
            yMin = 0
          else:  
            if float(yMin) < plt_y_min:
              yMin = 0            
            
        yMax = float(yMax)
        yMin = float(yMin)    
        ind = 0        
        while ind < len(xp):
          if (float(xp[ind][1]) > yMax) or (float(xp[ind][1]) < yMin):
            print("Magnifier click discarded - ",xp[ind])
            xp.pop(ind)
          ind+=1  

# Get the start and end times of the data of interest (in the right datetime format). When reading through a data file these are used
# to decide when to start and stop processing data records.
  fst = True  
  pds = []
  ind = 0
  while ind < len(xp):
    dt = int(xp[ind][0] * 86400)
    timestamp = datetime.datetime.utcfromtimestamp(dt)
    tm = timestamp.strftime('%Y-%m-%d %H:%M:%S')
    if fst:
      fst = False
      strt = tm
      startTime = strt.replace("-",bchar)  
    else:
      px = strt + " - " + tm
      
      strt = tm
      pds.append(px)       
    ind+=1
  stopTime = strt.replace("-",bchar)  

# If the user was selecting the start and stop times of the data of interest that's now been done, so return.
  if trimTimes:
    return

# Show the user a list of (numbered) activities
  print(" ")
  options = setupList(activityList)
  for idx, element in enumerate(options):
    print("{}) {}".format(idx+1,element))

# Show the user the activity periods they defined by left clicking on the plot. For each one ask them to specify what the activity was
# (by entering the associated number from the list of activities). Store these in 'ivals' for use later when writing the output files.  
  ind = 0  
  print("\nSelect activity (by number) for each time period - \n")  
  uniqueAct = []
  for px in pds:
    done = False
    while not done: 
      txt = px + " - select activity (by number) - "    
      i = input(txt)
      try:
        if 0 < int(i) <= len(options):
          done = True
          endTime = px[22:].replace("-",bchar)
          ivals[ind][0] = endTime
          ivals[ind][1] = options[int(i)-1]
          if not ivals[ind][1] in uniqueAct:
            uniqueAct.append(ivals[ind][1])
          ind += 1 
      except:
        pass
  
  trimStart , trimEnd = plotGetStartEndTimes(filename,False)

# Read the csv file from the sensor into a dataframe
  df = pd.read_csv(filename,header=None,names=headers,skipfooter=trimEnd,engine='python',skiprows=trimStart)

# Remove any rows with invalid data entries ('NA')
  df.dropna(axis=0, how='any', inplace=True)

# Calculate the root mean square values of the x, y and z values, and add them to the dataframe
  rmsVals = getRmsVals(df)
  df['rms'] = rmsVals  

# create a list of dataframe values
  newVals = df.values.tolist()
  
  return

#--------------------------------------------------------------------------------------------------------------------------------------
# Provide the user with a graphical display of the processed data, including the root mean square values and the activities specified
# by the user/predited by a model. 
# Plot the output data file and wait for the user to close the plot before continuing with processing.
#--------------------------------------------------------------------------------------------------------------------------------------
def plot_raw_data2(newVals):
  plt.rcParams["figure.figsize"] = [14.50, 6.50]
  plt.rcParams["figure.autolayout"] = True
  headers = ['Time', 'x_axis', 'y_axis', 'z_axis', 'rms', 'activity']
  
  df = pd.DataFrame(newVals, columns = headers) 
  df['Time'] = pd.to_datetime(df['Time'])
  df['Time'] = df['Time'].dt.round('1s') 
  df.set_index('Time').plot()

  plt.xticks(rotation=90)
  plt.minorticks_on()
  plt.grid(b=True, which='major', axis='x', color='0.65', linestyle='-', lw = 1)  
  plt.grid(b=True, which='minor', axis='x', color='0.65', linestyle='--', lw = 1)  
  plt.show()

#--------------------------------------------------------------------------------------------------------------------
# Get the last datetime value in the input file. This is used for when the user trims data from the end of the file 
# (by subtracting a number of seconds from it, as specified by the user)
# Also get the datetime format - dd-mm-yyyy or yyyy-mm-dd and with separating character '-' or '/'
#---------------------------------------------------------------------------------------------------------------------
def get_datetime_format(filename):
  global sy, bchar
  file_handle = open(filename)
  file_size = file_handle.tell()
  file_handle.seek(max(file_size - 2*1024, 0))

# Get rid of trailing newlines
  last_1 = file_handle.read().splitlines()[-1:]
  last_1 = last_1[0][0:19]
  
  if "-" in last_1:
    bchar = "-"
  else:
    bchar ="/"  
# Find if '-' or '/' are used to separate parts of the date, eg 2022-05-14 or 2022/05/14    
  if last_1[4] == "-" or last_1[4] == "/":
    sy = True
  return last_1    

#---------------------------------------------------------------------------------------------------------------
# For manual labelling of an unlabelled file.
# Ask the user how many records they wish to retain per second for the output file. eg the sensor may have been 
# configured to make 200 readings per second, but we may find that just 50 readings per second are adequate for
# predicting activities from data.
#---------------------------------------------------------------------------------------------------------------
def getPerSec(freq):
    ok = False
    while not ok:    
      try:
        perSec = int(input("Specify number of readings to retain per second (min 5) - "))
        if (perSec > 4) and (perSec <= freq):
          ok = True
          return(perSec)
      except ValueError:
        ok = False
        
#----------------------------------------------------------------------------------------------------------------
# For manual labelling of an unlabelled file.
# Ask the user for the subjectid (eg the initials of the person performing the activity). This is incorporated in 
# the output filename, and may be a useful piece of information to have when analysing results.   
#----------------------------------------------------------------------------------------------------------------     
def getSubjectId():
    ok = False
    while not ok:    
        subjectId = input("\nEnter subject id (eg initials) - ")
        if len(subjectId) > 0:
          ok = True
          return(subjectId)

#--------------------------------------------------------------------------------------------------------------------------------
# Ask the user for the name of the file to create for holding data that has been labelled using predictions provided by a model.
# If it already exists  ask if they want to overwrite it, choose another file or stop.
#--------------------------------------------------------------------------------------------------------------------------------
def getNewFileName(xfilename,itxt):
    ok = False
    while not ok:
        txt = "\nEnter a suffix to add to " + xfilename + " for use as the output file name, eg " + itxt + " - "
        predictedFile = input(txt)
        if len(predictedFile) > 0:
          predictedFile = xfilename[:-4] + predictedFile + ".csv"
          isfile = os.path.isfile(predictedFile) 
          if isfile:
            ui = ""
            while ui != "C":
              ui = input("File already exists. Overwrite it (O), choose another name (C) or stop (S) : ").upper()    
              if ui == "S":
                print("\nTerminating ...")
                exit()
              if ui == "O":
                ok = True
                return(predictedFile)
          else:                
            ok = True
            return(predictedFile)

#-----------------------------------------------------------------------------------------
# Ask the user which folder to use to hold the kmeans/neural network model being created.
# If it exists ask the user if they wish to overwrite it, choose another folder or stop.
# If it doesn't exist create it.
#-----------------------------------------------------------------------------------------
def getFolderName(txt):
    currDir = os.getcwd()
    ok = False
    while not ok:
        inFolder = input(txt)
        if len(inFolder) > 0:
          path = currDir + "/" + inFolder
          isdir = os.path.isdir(path) 
          if isdir:
            ui = ""
            while ui != "C":
              ui = input("Directory already exists. Overwrite it (O), choose another name (C) or stop (S) : ").upper()    
              if ui == "S":
                print("\nTerminating ...")
                exit()
              if ui == "O":
                ok = True
                return(path)
          else: 
            os.mkdir(path)
            ok = True
            return(path)

#----------------------------------------------------------------------------------------------------------------------
# tfList contains a random list of True and False values, used to select entries from the input file according to 
# how many entries per second the user specified they wished to retain (for the output file). eg if the input file
# contains 200 entries per second and the user wants to keep 50 per second then 75% of the values in tfList will be
# set to False and 25% to True (at random). When the data are being processed records will be used/discarded according
# to the values in tfList.       
def setTFList(perSec,freq) :
  sample = random.sample(range(0,freq), perSec)
  tfList = []
  for i in range(freq):
    if i in sample:
      tfList.append(True)
    else:
      tfList.append(False)
  return tfList   
#-----------------------------------------------------------------------------------------  

#-----------------------------------------------------------------------------------------------------------------------
# Get the means and standard deviations for the x,y and z values, and the root mean square (rms) of the three (for each
# block of activity)
#------------------------------------------------------------------------------------------------------------------------
def getStats():
  xMean = round(statistics.mean(xvals),2)
  yMean = round(statistics.mean(yvals),2)
  zMean = round(statistics.mean(zvals),2)
  rMean = round(statistics.mean(rms),2)
  xstdev = round(statistics.stdev(xvals),2)
  ystdev = round(statistics.stdev(yvals),2)
  zstdev = round(statistics.stdev(zvals),2)
  rstdev = round(statistics.stdev(rms),2)
  duration = round((ns/freq),2)
  return(xMean, yMean, zMean, rMean, xstdev, ystdev, zstdev, rstdev, duration)

#------------------------------------------------------------------------------------  
# Save the most recently read x,y,z and rms value for the current block of activity |
#------------------------------------------------------------------------------------  
def setVals():
  global xvals, yvals, zvals, rms , row , svm
  xvals.append(float(row[1]))  
  yvals.append(float(row[2]))        
  zvals.append(float(row[3])) 
  rms.append(svm) 

#-------------------------------------------------------------------------------------------------------------------
# Get the root mean square value (square root of x-squared + y-squared + z-squared) for each record in a dataframe |
#-------------------------------------------------------------------------------------------------------------------
def getRmsVals(df):
  X = df[['x_axis', 'y_axis', 'z_axis']]
  X = X.values
  rmsVals = []
  for x in X:  
    rms = (float(x[0])**2 + float(x[1])**2 + float(x[2])**2)**0.5
    rmsVals.append(rms)
  return rmsVals  
 
#---------------------------------------------------------------------------------------------------------------------
# Group together the x, y and z values into blocks of 'time_steps' and overlap blocks by 'time_steps - step' records.
# Assign the most common activity (from 'stats.mode') from the labels for all the records in a block, to that block.
# These blocks will be fed into the neural network for it to try and figure out patterns in the x, y, z values from
# training data to predict activities for test data.
#---------------------------------------------------------------------------------------------------------------------
def create_dataset(X, y, time_steps=1, step=1):
    Xs, ys = [], []
    for i in range(0, len(X) - time_steps, step):
        v = X.iloc[i:(i + time_steps)].values
        labels = y.iloc[i: i + time_steps]
        Xs.append(v)
# stats.mode returns the most common value (of activities)in the array (subset of x,y,z values and the associated
# activity) 
        ys.append(stats.mode(labels)[0][0])
    return np.array(Xs), np.array(ys).reshape(-1, 1)
    
#------------------------------------------------------------------------------------------------------------------------------
# When using a neural network model to predict activities from x, y and z values, group these values into blocks with the same
# number of records, and same overlap between blocks, as were used when the model was created.
#------------------------------------------------------------------------------------------------------------------------------
def create_dataset2(X, time_steps=1, step=1):
    Xs = []
    for i in range(0, len(X) - time_steps, step):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)
    return np.array(Xs)

#-------------------------------------------------------------------------------------------------------------------------------------
# kMeans and keras (the neural network) algorithms require their inputs to be numeric. The activity types (running, jumping etc) are 
# converted into numeric values to create these models. When they are used to predict activities for an unlabelled file they output
# numbers rather than text for the activity type. To create a labelled file from the model predictions the user is presented with a
# plot of the original data and predicted activities from which they associate the numerical activity value with the text value for
# the activity (eg 'walking'). 
#-------------------------------------------------------------------------------------------------------------------------------------    
def getActivityMap(aList,uniqueAct,activities):
  print(" ")
  ixx = len(aList)*[-1]
  for a in uniqueAct:
    txt = "Plot value " + str(a) + " - "
    ix = -1
    while ix < 0:
      try:
        ix = int(input(txt))
        if ix < 0 or ix > (len(aList)-1):
          print("You must enter a number between 0 and ",str(len(aList)-1))    
          ix = -1
        if ix in ixx:
          print("You have already selected this activity")    
          ix = -1
        if ix > -1:
          ixx[a] = ix
      except ValueError:
        print("You must enter a number between 0 and ",str(len(aList)-1))    
        ix = -1  
        
  actualAct = []
  for val in activities:
    actualAct.append(aList[ixx[val]])
      
  return actualAct        

#--------------------------------------------------------------------------------------------------------------------------------------------
# When a model is created the list of activities it was created from is written (in this function) into a file called 'activities' in
# the same folder as the model is created. If a user subsequently uses this model to label an unlabelled file (from the sensor) they are 
# shown which activities the model was created with - as a check that this is an appropriate model to use for labelling the file (by reading
# the 'activities' file).
#--------------------------------------------------------------------------------------------------------------------------------------------
def writeActivitiesFile(actFile,stepInfo,TIME_STEPS,STEP,activities):
  with open(actFile, 'w') as f: 
    write = csv.writer(f) 
    if stepInfo:
      write.writerow([TIME_STEPS])   
      write.writerow([STEP])   
    write.writerows([activities])   
  f.close()  

#-----------------------------------------------------------------------------------------------------------------------------------------
# When a model is used to label an unlabelled file, show the user a plot of the input data and the associated activities predicted by the 
# model.    
#-----------------------------------------------------------------------------------------------------------------------------------------
def plotPredictedActivities(activities,newVals):
  ix = 0
  for p in activities:  
    newVals[ix].append(p)
    ix+=1
  print("\nPlot of original data and predicted classes - ")
  plot_raw_data2(newVals)
  
#----------------------------------------------------------------------------------------------------------------------------------
# When a model is used to predict activities for an unlabelled file from the sensor, show the user a text summary of the predicted
# activitites, with the duration for each activity block in seconds, eg
# clap - 2 seconds
# walk - 300 seconds
# clap - 4 seconds
# walk - 6 seconds
# jump - 3 seconds
# clap - 2 seconds
#----------------------------------------------------------------------------------------------------------------------------------
def predictedActivitySummary(activities,uniqueAct,freq):
  global multVal
  print("\nPredicted activity summary :\n")
  ind = 0
  acPrev = activities[0]
  actNums = []
  for ac in activities:
    actNums.append(uniqueAct.index(ac))
    if ac != acPrev:
      print(acPrev," - ",round((ind/freq)*multVal,1)," seconds")
      acPrev = ac
      ind = 0
    else:
      ind+=1
  if ind > 0:
    print(acPrev," - ",round((ind/freq)*multVal,1)," seconds")
  return actNums  

#----------------------------------------------------------------------------------------------------------------------------------------
# For an unlabelled file from the sensor, show the user a plot of the data and ask them to select the data of interest by left clicking
# the start and stop times - to remove data from the start and end of the file that we are not interested in, and that may confuse a
# model that is predicting activity (when the user is attaching the sensor to themselves, eg putting it on their wrist), and removing
# it when they have finished. 
# Also add root mean square values of the x, y and z readings, and scale the data (so large values for one variable don't produce 
# an inappropriate level of influence on a model that the data is fed into).
#----------------------------------------------------------------------------------------------------------------------------------------
def getUnlabelledData(xfilename):
  global multVal
  print("\nSelect the time period to process for the unlabelled csv file")
  last_1 = get_datetime_format(xfilename)

  trimStart , trimEnd = plotGetStartEndTimes(xfilename,True)
  
  column_names = ['timestamp','x_axis','y_axis','z_axis']

# Read the csv file from the sensor into a dataframe
  df = pd.read_csv(xfilename,header=None,names=column_names,skipfooter=trimEnd,engine='python',skiprows=trimStart)

# Remove any rows with invalid data entries ('NA')
  df.dropna(axis=0, how='any', inplace=True)

  dflen = len(df)
  numToProc = getNumToProc(dflen)
  dFrac = round(numToProc/dflen,2)

  multVal = 1/dFrac  
  df = df.sample(frac=dFrac).sort_index()
 
  newVals = df.values.tolist()

  rmsVals = getRmsVals(df)

  ix = 0
  for rms in rmsVals:  
    newVals[ix].append(rms)
    ix+=1
  
  df = scaleInputData(df)
  
  return df, newVals, rmsVals, trimStart, trimEnd

#---------------------------------------------------------------------------------------------------------------
# Ask the user how many records they wish to process from the specified input file.
#---------------------------------------------------------------------------------------------------------------
def getNumToProc(dflen):
    print("\nThere are ",dflen," records in the input data. How many do you wish to randomly select (CR for all records; min 1000).")
    ok = False
    while not ok:    
      try:
        numToProc = input(" - ")
        if is_int(numToProc):
          numToProc = int(numToProc)
          if (numToProc <= dflen) and (numToProc > 1000):
            ok = True
        elif len(numToProc)==0:
          numToProc = dflen
          ok = True 
      except ValueError:        
        ok = False
    return(numToProc)

def getFileSample(xfilename):
  headers = ['Time', 'x_axis', 'y_axis', 'z_axis', 'rms', 'activity']
  df = pd.read_csv(xfilename,header=None,names=headers,engine='python')

# Remove any rows with invalid data entries ('NA')
  df.dropna(axis=0, how='any', inplace=True)

  dflen = len(df)
  numToProc = getNumToProc(dflen)
  dFrac = round(numToProc/dflen,2)

  df = df.sample(frac=dFrac).sort_index()
  return df

def getSampleContents(df):
  activities = []
  actlist = []
  tListx = []
  for ind in df.index:
    actlist.append(df['activity'][ind])
    tListx.append([df['x_axis'][ind],df['y_axis'][ind],df['z_axis'][ind]])
  activities = list(set(actlist))
  return activities, actlist, tListx

  
#-------------------------------------------------------------------------------------------------------------------------------------------
# Use kMeans clusters to predict the label (activity) for each row in the output csv file. 
# Compare with the actual labels.
# Explanation - https://www.datacamp.com/tutorial/k-means-clustering-python
# The parameters that maybe useful to vary, or definitely must be set are :
#   n_clusters : we will set this to the number of different activities in the input data file. We aim to assign each data point to one of 
#     these activities, so this is the number of clusters we want.
#   max_iter - the maximum number of iterations performed. 
#    From - https://stats.stackexchange.com/questions/261836/k-means-how-many-iterations-in-practical-situations
#    in all practical situations a maximum of 20-50 is sufficient. The default value is 300 so we'll simply leave it at that.
#   n_init - the number of times the algorithm is run with different centroid seeds. Default is 10.
#   tol - the relative tolerance of the difference in cluster centres of two consecutive iterations to declare convergence. Default 1e-4. 
# Other parameters are largely concerned with efficiency/memory usage.
# After creating the model, store it (in a place specified by the user), along with an 'activities' file (containing a list of the 
# activities analysed.
#-------------------------------------------------------------------------------------------------------------------------------------------
def kmeansAnalysis(xfilename):
  df = getFileSample(xfilename)
  activities, actlist, tListx = getSampleContents(df) 

#  with open(xfilename, 'r') as csvFile:
#    reader = csv.reader(csvFile)
#    for row in reader:
#      actlist.append(row[5])
#      tListx.append([row[1],row[2],row[3]])
#      if not row[5] in activities:
#        activities.append(row[5])
#  csvFile.close()

  numact = len(activities)

# The model outputs predictions as numeric values. Compare all combinations of the actual values held in the input file against 
# the predictions made by the model to see which provides the best fit. eg if the input file contained :
# clap, clap, walk upstairs, walk upstairs, walk upstairs, clap, clap, walk downstairs, walk downstairs, walk downstairs, clap, clap
# and the model predictions were :
# 0, 1, 1, 1, 0, 2, 2, 2, 0, 1, 0
# we see how many matches are made for eg - 
# 0 = clap, 1 = walk upstairs, 2 = walk downstairs 
# 0 = clap, 2 = walk upstairs, 1 = walk downstairs
# 1 = clap, 0 = walk upstairs, 2 = walk downstairs 
# etc, checking all combinations and outputing the one that gives the most matches with trhe actual data.
   
  itx = []
  for item in permutations(range(numact), numact): 
    itx.append(item)
    
  X = np.array(tListx)
  
  ok = ""
  while ok != "Y" and ok != "N":
    ok = input("\nScale input data? Y/N ").upper()
    if ok == "Y":
      scaler = MinMaxScaler()
      X_scaled = scaler.fit_transform(X)
      kmeans = KMeans(n_clusters=numact, random_state=0).fit(X_scaled)
    elif ok == "N":  
      kmeans = KMeans(n_clusters=numact, random_state=0).fit(X)
  
  acts = smoothActivities(kmeans.labels_)  
  
  maxMatches = 0
    
  for itxx in itx:
    ind = 0
    mtch = 0
    for la in acts:
      for ix in range(numact):
        if (activities[ix] == actlist[ind]) and (la == itxx[ix]):
          mtch += 1
      ind+=1
    if mtch > maxMatches:
      maxMatches = mtch
      bestMatch = itxx
      
  print("Bestmatch - ",bestMatch)
  print("Matches - ",maxMatches,"/",ind)  
  print(round((maxMatches/ind)*100,1),"% accuracy")
  
  
  print("\nSelect parent folder for containing the kMeans Model sub folder - ")
  selectFolder()
  
  path = getFolderName("\nEnter the name of the sub folder to use to store the kMeans model (eg NSLW02Feb2022) - ")
    
  print("\nSaving model\n")
  pklFile = path + "/kMeans.pkl"
  pickle.dump(kmeans, open(pklFile, "wb"))
  actFile = path + "/activities.csv" 
  writeActivitiesFile(actFile,False,0,0,activities)
  exit()

#-----------------------------------------------------------------------------------------------------------------------------
# Combine multiple copies of a machine learning model, that have been trained independently, to provide an average model.
# Check the resultant model against the test data and show the user it's accuracy (how well predicted activities match actual
# activities in the test data).
# Save the model (to a folder that's been specified by the user). 
#-----------------------------------------------------------------------------------------------------------------------------  
def baggingModelProc(filename,bagged_mod,modName,mod3,X_train, y_train,X_test,y_test,outFileEnd):
  bagging_model = BaggingClassifier(bagged_mod, n_estimators=100)
  bagging_model.fit(X_train, y_train)

# Pass the test data to the trained model to obtain predictions (of activity), and compare the accuracy with the actual activity values  
  predictedAct = bagging_model.predict(X_test)
  accuracy = round(accuracy_score(y_test, predictedAct) * 100,1)
  print(modName," accuracy - ",accuracy) 
  
  filename = mod3 + '_' + outFileEnd
  print("Saving model to ",filename)  
  pickle.dump(bagging_model, open(filename, 'wb'))

#--------------------------------------------------------------------------------------------------------------------------------------        
# Create 5 different machine learning models, each one trained against a data file from the sensor that has been manually
# labelled with activities by a user (based on knowing what the actual activities were that were performed when the data file was
# being created). 
# Each model has inputs (called hyperparameters) that can be varied to try to maximise the accuracy of predictions.
# The 'gridsearch' utility evaluates a model for each combination of these inputs, within specified ranges, to find the optimum values
# (in terms of prediction accuracy). It uses randomly selected partitions of the data to assess each combination.
# Each model is then trained against a 70% portion of the input file, with these optimum parameters, and then assessed for accuracy 
# against a 30% proportion of the file.
# The user is shown the accuracy of the model (as measured against the test data), and the model is saved (at a location specified by
# the user).
# Information from - https://machinelearningmastery.com/evaluate-machine-learning-algorithms-for-human-activity-recognition/
#--------------------------------------------------------------------------------------------------------------------------------------        
def machineLearnAnalysis (xfilename):
  print("""
Select folder for saving Machine Learning models to.
Each model will be saved to a file in this folder using a filename made up of an abbreviation for the model and the
name of the input file from which the model is generated.
eg If the input file is 80933_0000000004LWNS.csv, then the following saved model files will be created :
  knn_80933_0000000004LWNS.sav (k-Nearest-Neighbour)
  rft_80933_0000000004LWNS.sav (Random_Forest)
  dtc_80933_0000000004LWNS.sav (Decision_Tree)
  svc_80933_0000000004LWNS.sav (Support_Vector_Machine)
  gnb_80933_0000000004LWNS.sav (GaussianNB)
  """)
  
  selectFolder() 

  outFileEnd = os.path.basename(xfilename)[:-4] + ".sav"
  
  print("\nPlease wait, producing machine learning models ...\n")

  df = getFileSample(xfilename)
# remove the timestamp and root mean square columns - they are not used by these algorithms
  df = df.drop("Time", axis=1)
  df = df.drop("rms", axis=1)

  df = scaleInputData(df)
# Select 20% of the dataframe, keeping the proportion of each activity the same as for the whole dataframe.
# This will be used for tuning hyperparameters for algorithms that take a long time to get the tuning values for.  
  part_df = df.groupby('activity').apply(lambda x: x.sample(frac=0.2))

# The independent variables (x, y and z acceleration values) are loaded into 'X', and the dependent variable (activity - what we're trying to
# predict from the accelerometer outputs) is loaded into 'y'
  X = df.drop("activity", axis=1)
  X = X.values
  y = df["activity"]
  y = y.values
  uniqueAct = list(set(y))
  
# Split the data into train and test blocks, 70% train and 30% test 
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

  pX = part_df.drop("activity", axis=1)
  pX = pX.values
  py = part_df["activity"]
  py = py.values
  
# Split the partial data blocks into train and test blocks, 70% train and 30% test 
  pX_train, pX_test, py_train, py_test = train_test_split(pX, py, test_size=0.3)
# Random forest information - https://medium.com/analytics-vidhya/random-forest-classifier-and-its-hyperparameters-8467bec755f6 
# Tuning - https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d#:~:text=the%20test%20performance.-,max_depth,the%20training%20and%20test%20errors. 
  param_grid = {'n_estimators': [32,64,128], 'criterion': ['gini','entropy'],'min_samples_leaf': range(1,5,1),'min_samples_split': range(2,6,1),'max_features': ['auto', 'log2']}
# Find the optimum hyperparamaters for Random Forest
  gridsearch = GridSearchCV(RandomForestClassifier(),param_grid,cv=5,n_jobs=-1,verbose=0)
  gridsearch.fit(pX_train,py_train)
  best_ne = gridsearch.best_params_['n_estimators']  
  best_crit = gridsearch.best_params_['criterion']  
  best_msl = gridsearch.best_params_['min_samples_leaf']  
  best_mss = gridsearch.best_params_['min_samples_split']  
  best_mf = gridsearch.best_params_['max_features']  
  
# Train the Random Forest model
  rand_rfc = RandomForestClassifier(n_estimators=best_ne,criterion=best_crit,max_features=best_mf,min_samples_leaf=best_msl,min_samples_split=best_mss)
  rand_rfc.fit(X_train, y_train)

# Pass the test data to the trained model to obtain predictions (of activity), and compare the accuracy with the actual activity values  
  predictedAct = rand_rfc.predict(X_test)
  accuracy = round(accuracy_score(y_test, predictedAct) * 100,1)
  print("Random Forest Classifier accuracy - ",accuracy) 

# Save the trained model  
  filename = 'rfc_' + outFileEnd
  print("Saving model to ",filename)  
  pickle.dump(rand_rfc, open(filename, 'wb'))
       
# k-Nearest neighbour is a supervised machine learning algorithm, based on the concept that data with similar characteristics are liable to 
# belong to the same categories (when aiming to categorize unseen data). 
# The two optional parameters that affect the accuracy of k-Nearest neighbour are how many neighbouring points are 
# taken into account, and whether or not the distance of neighbouring points from the point under consideration is weighted.
  parameters = {"n_neighbors": range(1, 50),"weights": ["uniform", "distance"]}
  gridsearch = GridSearchCV(KNeighborsClassifier(), parameters)
  gridsearch.fit(X_train, y_train)  
  best_k = gridsearch.best_params_["n_neighbors"]
  best_weights = gridsearch.best_params_["weights"]

# Bagging trains the model on different subsets of the training data, and combines predictions based on a combination of results. 
# Save the resulting model. 
  bagged_knn = KNeighborsClassifier(n_neighbors=best_k, weights=best_weights) 
  baggingModelProc(filename,bagged_knn,"k-Nearest Neighbour","knn",X_train,y_train,X_test,y_test,outFileEnd)
  
# A decision tree builds a set of yes/no rules based on the training data to enable it to allocate a category (in this case) for unseen 
# data points (from the test data). The model determines the best set of rules to apply to maximise the accuracy of predictions based on 
# the target values (activity type) contained in the training data. With each split an evaluation is made of the number of data points
# belonging to each class (activity type here). 
# Explanation - https://towardsdatascience.com/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b60575

# Optimize (tune) the hyperparameters - see https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680  
# Features of the tree that can be changed to affect accuracy are : 
#   the function used to measure the quality of the split
#   the depth of the tree (how many splits are made)
#   the minimum number of samples any branch can terminate with

  param_grid = [{'min_samples_leaf': range(1, 20)},{'min_samples_split': range(2, 40)}]

# Call the fit() method to perform the grid search using 3-fold cross-validation.
  gridsearch = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=3)  
  gridsearch.fit(X_train, y_train)

# Bagging trains the model on different subsets of the training data, and combines predictions based on a combination of results. 
# GridsearchCV will either return min_samples_leaf or min_samples_split. Use whichever it is as the tuning value. 
  if 'min_samples_leaf' in gridsearch.best_params_ :
    best_msl = gridsearch.best_params_['min_samples_leaf']  
    bagged_dtc = DecisionTreeClassifier(min_samples_leaf=best_msl)
  else:
    best_mss = gridsearch.best_params_['min_samples_split']  
    bagged_dtc = DecisionTreeClassifier(min_samples_split=best_mss)

  baggingModelProc(filename,bagged_dtc,"DecisionTreeClassifier","dtc",X_train,y_train,X_test,y_test,outFileEnd)
  
# SVC is Support Vector Machine (SVM) algorithm applied for classification. SVM finds hyperplanes that distinctly classify data. A hyperplane
# acts as a decision boundary for classifying data points. The algorithm looks for the hyperplane that provides the greatest distance between 
# data points with different classifications on either side of it, so the separation between the 2 classes is as wide as possible.  
# This maximises the probability that unseen data points are assigned to the correct class.  
# Explanation - https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47  
# and - https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989#:~:text=SVM%20or%20Support%20Vector%20Machine,separates%20the%20data%20into%20classes.
# The most commonly adjusted params (according to https://holypython.com/svm/support-vector-machine-optimization-parameters/) are :
#  Cfloat , kernel, degree, gamma, tol and cache_size.
# C - as 'c' increases the intricacy of the decision curve separating classes increases. More training points are classified correctly,
#     but generalisation decreases, which may result in poorer predictions for unseen data (overfitting). Default 1.0.
# Gamma - as this increases more weight is given to points close to the boundary potentially causing overfitting. A low value gives more 
#         weight to points further from the boundary, potentially underfitting. 
# kernel - the function type used to transform data to try to obtain the best distinction between classes. One of linear, poly, rbf,
#          sigmoid, precomputed. Default rbf (radial basis function).
# degree - only applies to 'poly'. Default 3.

  param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}
  gridsearch = GridSearchCV(SVC(),param_grid,refit=True)
  gridsearch.fit(pX_train,py_train)
  best_C = gridsearch.best_params_['C']  
  best_gamma = gridsearch.best_params_['gamma']  
  best_kernel = gridsearch.best_params_['kernel']  

# Bagging trains the model on different subsets of the training data, and combines predictions based on a combination of results.  
  bagged_svc = SVC(C=best_C,gamma=best_gamma,kernel=best_kernel)
  
  baggingModelProc(filename,bagged_svc,"SVC","svc",X_train,y_train,X_test,y_test,outFileEnd)
  
  params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}
  gridsearch = GridSearchCV(GaussianNB(), param_grid=params_NB, cv=3, scoring='accuracy') 
  gridsearch.fit(X_train, y_train)

  best_vs = gridsearch.best_params_['var_smoothing']  

# Bagging trains the model on different subsets of the training data, and combines predictions based on a combination of results.  
  bagged_GNB = GaussianNB(var_smoothing=best_vs) 
  baggingModelProc(filename,bagged_GNB,"GaussianNB","gnb",X_train,y_train,X_test,y_test,outFileEnd)
  
  actFile = "activities.csv"
  writeActivitiesFile(actFile,False,0,0,uniqueAct)
  exit()

#---------------------------------------------------------------------------------------------------------------------------------------------------------
# Load a (manually) labelled data file from the sensor into a dataframe. Split it into train and test data sets for passing to a neural 
# network model. 
# Scale the data, so that large values in one variable don't cause that variable to have a disproportionate influence on the model.
# Form blocks of data (defined by a time period specified by the user - default 2 second blocks), that are overlapped (
# by a percentage specified by the user - default 50%).
# Convert the activity values (held as text in the input file, eg 'walk upstairs') into numerical values, as required by the neural 
# network.
# Info -  https://towardsdatascience.com/time-series-classification-for-human-activity-recognition-with-lstms-using-tensorflow-2-and-keras-b816431afdff
#---------------------------------------------------------------------------------------------------------------------------------------------------------
def load_dataset (xfilename):
  global X_train , X_test , y_train , y_test
# Specify the column names for the output csv file - the recorded datetime, x, y and z values; the calculated rms and the
# associated activity (as specified by the user)

  df = getFileSample(xfilename)

# The neural network seems to 'learn' best if the data are not in large blocks, all with the same activity - if different activities are
# mixed up. Check for any such large blocks (the same activity for more than 10 seconds). If there are any, shuffle the data randomly, in 10
# second blocks.
    
# Find the mean number of data records per second after random sampling has occured
  dflen=len(df)
  freqs=[]
  freq=0
  oldsec=df.iloc[0][0][0:19]
  for ix in range(dflen):
    sec=df.iloc[ix][0][0:19]
    if sec==oldsec:
      freq+=1
    else:
      freqs.append(freq)
      freq=0
      oldsec=sec
  freq=(math.ceil(mean(freqs)))

# See if there are any blocks of data spanning more than 10 seconds (based on the mean number of records per second just calculated) that all
# have the same activity category
  maxRecs = freq*10
  ind=0
  shuffleRecs=False

  for i in range(len(df)-1):
    if df.iloc[i]['activity'] == df.iloc[i+1]['activity']:
      ind+=1
      if ind > maxRecs:
        shuffleRecs=True
        break
    else:
      ind=0

#There is at least one block of data of more than 10 seconds all with the same activity. Split the data into (roughly) 10 second blocks
# and randomly insert them into a new dataframe
  if shuffleRecs: 
    fRec=10*freq 
    numBlocks = int(dflen/fRec)

    sel = random.sample(range(numBlocks), numBlocks)
    data = []
    for s in sel:
      x1 = s*fRec
      x2 = x1+fRec
      xBlock = df.iloc[x1:x2].values.tolist()
      data.append(xBlock)

    flat = [x for sublist in data for x in sublist]
    headers = ['Time', 'x_axis', 'y_axis', 'z_axis', 'rms', 'activity']    
    df = pd.DataFrame(flat,columns=headers)

# Split the dataframe into test and train entities - first 70% for training, last 30% for testing. Train and test 
# should contain examples of all activities.
  df_train, df_test = np.split(df, [int(.7*len(df))])
  
  activityList = df_train.activity.unique()

  ok = ""
  while ok != "Y" and ok != "N":
    ok = input("\nScale input data? Y/N ").upper()
    if ok == "Y":
# The data are scaled so that larger values for one variable don't have an undue influence on the machine learning 
# algorithm, resulting in the values of other variables being under-represented in the processing.
      scale_columns = ['x_axis', 'y_axis', 'z_axis']
      scaler = RobustScaler()
      scaler = scaler.fit(df_train[scale_columns])
      df_train.loc[:, scale_columns] = scaler.transform(df_train[scale_columns].to_numpy())
      df_test.loc[:, scale_columns] = scaler.transform(df_test[scale_columns].to_numpy())
    elif ok == "N":
      columns = ['x_axis', 'y_axis', 'z_axis']
      df_train.loc[:, columns] = df_train[columns].to_numpy()
      df_test.loc[:, columns] = df_test[columns].to_numpy()
        
  last_1 = get_datetime_format(xfilename)
#  freq = find_frequency(xfilename)

# Get the user to specify the length of time blocks that the data is split into in seconds
  i = 0
  while i == 0:
    i = input("\nEnter length of interval data is split into in seconds, eg 2.5 (CR for default = 2.0 seconds) : ")
    if len(i) == 0:
      i = 2
    elif is_int(i) or is_float(i):
      i = float(i)
      done = True
    else:
      i = 0
    
  TIME_STEPS = int(i*freq)
  
 # Get the user to specify the percentage overlap between blocks of data
  i = -1
  while i < 0:
    i = input("\nEnter percentage overlap between blocks of data in the range 0 to 90 (CR for default = 50%) : ")
    if len(i) == 0:
      i = 50
    elif is_int(i) or is_float(i):
      i = float(i)
      if i > 90:
        i = -1
      else:  
        done = True
    else:
      i = -1
   
  print(" ") 
  STEP = int(TIME_STEPS - ((i/100)*TIME_STEPS))
   
  X_train, y_train = create_dataset(
    df_train[['x_axis', 'y_axis', 'z_axis']],
    df_train.activity,
    TIME_STEPS,
    STEP
  )

  X_test, y_test = create_dataset(
    df_test[['x_axis', 'y_axis', 'z_axis']],
    df_test.activity,
    TIME_STEPS,
    STEP
  )    

# Convert the activity values which are strings (eg 'walk upstairs') to numerical values for use in the predictive
# model (keras, which works with numbers rather than strings). An array is created so that the model doesn't interpret
# different activities as having greater numerical differences between them, eg there is no distinction for the
# difference in the numeric values representing 'walk upstairs' and 'clap' and 'walk downstairs' and 'clap'.
  enc = OneHotEncoder(handle_unknown='ignore', sparse=False)
  enc = enc.fit(y_train)
  y_train = enc.transform(y_train)
  y_test = enc.transform(y_test)
  
  return activityList , TIME_STEPS , STEP
  
#------------------------------------------------------------------------------------------------------------------------------------
# After a machine learning/neural network model has been run against an unlabelled file from the sensor, create a new labelled file
# consisting of the input data, plus the root mean square value and the predicted activity for each data record.
# If the predicted activity from the model is numeric, get the user to specify the text value for each numeric output after showing
# them a plot of the input data and predicted activities.
# The user specifies where to create the labelled file.
#------------------------------------------------------------------------------------------------------------------------------------  
def produceLabelledFile(xfilename,getAssoc,aList,uniqueAct,activities,trimEnd,trimStart,rmsVals,txt):
  ok = ""
  print(" ")
  while ok != "Y" and ok != "N":
    ok = input("Produce labelled file using these predictions? Y/N ").upper()
    if ok == "Y":
      predictedFile = getNewFileName(xfilename,txt)
      
      if getAssoc:      
        print("\nSelect the actual activity (by number) associated with the activity values shown in the plot : \n")
        ind = 0
        for a in aList:
          print(ind," - ",a)
          ind+=1
        actualAct = getActivityMap(aList,uniqueAct,activities) 
      else:
        actualAct = activities 
        
      column_names = ['timestamp','x_axis','y_axis','z_axis']       
      df = pd.read_csv(xfilename,header=None,names=column_names,skipfooter=trimEnd,engine='python',skiprows=trimStart)
      df = df.head(len(actualAct))
      df['rms'] = rmsVals[0:len(actualAct)]  
      df['activity'] = actualAct     
      print("\nWriting to ",predictedFile)
      df.to_csv(predictedFile, index=False)  

#--------------------------------------------------------------------------------------------------------------------------------
# Get the start and end times of the data of interest for a given input file. 
# Show the user a plot of the data and ask them to left click the start and end times of the data to be processed.
# When the user puts the sensor on, and takes it off, there will be short time periods during these processes for which the data 
# is not useful, and which may lead to worse predictions from models if this data is included.
#---------------------------------------------------------------------------------------------------------------------------------
def plotGetStartEndTimes(xfilename,getInput):
  global startTime, stopTime
  if getInput:
    plot_raw_data(xfilename,True,'Select start/end times of the data of interest using mouse left-click. Finish inputs with mouse right-click. Remove entry with delete/backspace.')
  trimStart , trimEnd = 0 , 0
  with open(xfilename, 'r') as csvFile:
    reader = csv.reader(csvFile)
    for row in reader:  
# The data might be held as yyyy-mm-dd or dd-mm-yyyy. Process accordingly.    
      if sy:
        iTime = row[0][0:4] + bchar + row[0][5:7] + bchar + row[0][8:19] 
      else:
        iTime = row[0][6:10] + bchar + row[0][3:5] + bchar + row[0][0:2]+ row[0][10:19]
      if (iTime < startTime):  
        trimStart+=1
      elif (iTime > stopTime):  
        trimEnd+=1
  return trimStart , trimEnd      


#-----------------------------------------------------------------------------------------------------------------------------------------
# Build a 'keras' neural network model using input data that has been manually labelled with activities by the user.
# The input data is split into 70% training data, and 30% testing data (for evaluation of accuracy). 
# Each set of data is split into overlapping blocks with a fixed time period  - the time period length and overlap % can be specified
# by the user - default 2 seconds and 50%.
# A neural network consists of 'nodes' (to emulate neurons in a brain) with data being passed from one node to the next according to 
# whether or not its value exceeds a certain threshold when subjected to a particular transformation. Blocks of data are combined and the 
# model determines how well it is performing in terms of making predictions as compared to the actual target values.
# An attempt was made to use 'gridsearch' as for the machine learning algorithms, to tune the hyper-parameters for the model,
# but this resulted in poor accuracy. A 'manual' tune process is performed here, by testing the accuracy of predictions
# for a range of epochs (the number of times the data passes thrugh the model) and a range of batch sizes (the number of data
# records passed from one node to the next).
# A model is then built using the optimum epoch and batch size value, and saved to a location specified by the user.
# Info - https://machinelearningmastery.com/evaluate-machine-learning-algorithms-for-human-activity-recognition/
# Info on keras tuning - https://www.tensorflow.org/tutorials/keras/keras_tuner
#------------------------------------------------------------------------------------------------------------------------------------------
def kerasAnalysis (xfilename):
  global X_train , X_test , y_train , y_test
  tf.get_logger().setLevel('ERROR')

# Get the input file, loaded into overlapping blocks of data
  activities , TIME_STEPS , STEP = load_dataset(xfilename)

# Create a deep learning keras model that accepts the inputs (modified x,y,z values and associated (transformed) 
# activities) sequentially
  model = tf.keras.Sequential()
  model.add(
# allow the model to take into account previous as well as subsequent values being passed to it
    tf.keras.layers.Bidirectional(
# LSTM (Long short-term memory) is a type of recurrent neural network (RNN) that uses past information over a relatively
# long time period (compared to some neural network models) for improving prediction accuracy.    
      tf.keras.layers.LSTM(
# Create 128 copies of the RNN that are initialised with different weights (multipliers for the input values - different 
# ones are used to try to get the bestmeans of matching input values (x,y,z) to output values (activity)       
          units=128,
# Tell the model the dimensions (how many rows and columns here) are in each chunk of information being passed to it          
          input_shape=[X_train.shape[1], X_train.shape[2]]
      )
    )
  )
# Dropout removes a proportion of inputs. This helps prevent the model from overfitting earlier inputs (trying to get 
# an exact match between inputs and output rather than an approximate one) at the expense of later inputs (which may 
# contain data features of relevance that are subdued or not present in earlier inputs).
  model.add(tf.keras.layers.Dropout(rate=0.5))
# A Rectified Linear Activation function (relu) is applied by the nodes (neurons) to determine output values. 'relu'
# outputs either the input value if it is greater than zero, or zero otherwise. This results in many zero outputs which
# reduce the size of the matrices being processed and has been determined to be an effective function for pattern
# recognition.
  model.add(tf.keras.layers.Dense(units=128, activation='relu'))
# 'softmax' converts a vector of output values to a probability distribution enabling the output result to be interpreted 
# in terms of probability.
  model.add(tf.keras.layers.Dense(y_train.shape[1], activation='softmax'))

# categorical_crossentropy is used for models predicting categories where there are multiple possible outputs (here 
# the activities eg walk upstairs, walk downstairs, clap, etc). It is a 'loss function' - it seeks to minimise a
# particular scalar value to make predictions as close to the true labels as possible.
# The optimizer is an algorithm that changes the weights and learning rate to minimise losses (maximise the accuracy
# of predictions)
# The metric measures the performance of the model. 'acc' is for how often the prediction matches the category.
  model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['acc']
  )

# Train the model for a fixed number of iterations (epochs) on the dataset (here, the x,y,z values)
# batch_size is the number of input values passed into the model at a time. Different values should be tried to find
# the optimum value (best prediction rate) - there is not a formula for calculating the best batch_size to use.
# validation_split gives the fraction of training data used to validate the model - this proportion (here 10%) is
# set aside and used to assess how the model is performing after each epoch.
# shuffle is used to state whether or not the training data should be shuffledbefore being fed into the model. There is
# no benefit to doing that here - in fact the model may work better if values that are adjacent time-wise are kept in 
# correct order (for detecting repetitive patterns).
  eval_max = 0
  best_ix1 = 0
  best_ix2 = 0
    
  for ix1 in range(4,9):
    for ix2 in range(6,11):
      print("Checking epochs - ",ix1," batch_size - ",ix2)
      history = model.fit(X_train, y_train,epochs=ix1,batch_size=ix2,validation_split=0.1,shuffle=False,verbose=0)
# model.evaluate predicts outputs (here, for the test data) and computes all the metrics (we are interested in the
# accuracy of the activity predictions).
      eval = model.evaluate(X_test, y_test)
      if eval[1] > eval_max:
        eval_max = eval[1]
        best_ix1 = ix1
        best_ix2 = ix2

  print("Best epochs - ",best_ix1)
  print("Best batch_size - ",best_ix2)
  print("Match - ",eval_max)

# After showing the user how well the model performed, ask if they wish to save it.  
  ok = ""
  while ok != "Y" and ok != "N":
    ok = input("Save this Neural Network model? (Y/N) : ").upper()
    if ok == "N":
      print("\nTerminating")
      exit()

  history = model.fit(X_train, y_train,epochs=best_ix1,batch_size=best_ix2,validation_split=0.1,shuffle=False,verbose=0)

# model.predict simply shows the predictions (for the test data), but not the accuracy
#  y_pred = model.predict(X_test)
#  print("Predict - ",y_pred)
  
# Saving a trained Keras model (in files) for later use on other data :  
# https://www.tensorflow.org/tutorials/keras/save_and_load  
# Save the entire model as a SavedModel.
#  currDir = os.getcwd()

  print("\nSelect parent folder to create sub-folder in to store the Neural Network Model - ")
  selectFolder() 

  path = getFolderName("\nEnter the name of the sub-folder to use to store the Neural Network model (eg NSLW02Feb2022) - ")    
  print("\nSaving model\n")
  model.save(path)
  
  actFile = path + "/activities.csv" 
  writeActivitiesFile(actFile,True,TIME_STEPS,STEP,activities)
  exit()

#---------------------------------------------------------
# Get the user to select the folder containing the model |
#---------------------------------------------------------
def getModelFolder(modName,fName):
  ok = False
  txt = "\nSelect the folder containing the " + modName + " model - "
  txt2 = "\nThe two files " + fName + " and activities.csv must exist in the selected folder"
  while not ok:
    print(txt)
    directoryname = fd.askdirectory(title="select")    
    filename = directoryname + "/" + fName
    actFile = directoryname + "/activities.csv" 
    if not (os.path.exists(filename) and os.path.exists(actFile)):
      print(txt2)
    else:
      ok = True    
  return filename, actFile, directoryname    

#-------------------------------------------------------------------------------------------------------------------------------------  
# Run a previously saved 'kmeans' machine learning model against an unlabelled file from the sensor, and create a file containing the
# original data plus the root mean square value and predicted activity for each input record, if requested by the user.
# The user selects the input file and location of the model to load.
#-------------------------------------------------------------------------------------------------------------------------------------  
def runSavedKmeans(xfilename):
# Get the user to select the folder containing the saved model - a separate Explorer window opens for selection
  filename, actFile, directoryname = getModelFolder("kMeans","kMeans.pkl")

# Show the user what activities were contained in the data that was used to create the model.  
  aList, TIME_STEPS , STEP = showModelActivities(actFile, False)

# Load the input data
  df, newVals, rmsVals, trimStart, trimEnd = getUnlabelledData(xfilename)
  
  X = df.drop("timestamp", axis=1)
  X = X.values

# load the model
  new_model = pickle.load(open(filename, 'rb'))

# Get the activity predictions using the model
  predictedAct=new_model.predict(X)  

# 'Smooth' the predictions - for each 2 second interval set all the activity values to the most common value for that period.
# The model might for example predict 1/10th of a second of clapping during a 10 second period of walking - it is assumed that activities
# are performed in minimum time blocks of at least 2 seconds.
  activities = smoothActivities(predictedAct)  
  uniqueAct = list(set(activities))

# Show the user a summary of predicted activities
  actNums = predictedActivitySummary(activities,uniqueAct,freq)

# Display a plot of the original data and the predicted activities  
  plotPredictedActivities(activities,newVals)

# Create an output file containing the original data, plus root mean square values and predicted activities.
  produceLabelledFile(xfilename,True,aList,uniqueAct,activities,trimEnd,trimStart,rmsVals,"km01Jul2022")

  exit()

#-------------------------------------------------------------------------------------------------------------------------------------  
# Run a previously saved machine learning model against an unlabelled file from the sensor, and create a file containing the
# original data plus the root mean square value and predicted activity for each input record, if requested by the user.
# Any one of the 5 model types created by the code can be used - Random Forest, k-Nearest neighbour, support vector machine, decision tree
# and GaussianNB.
# The user selects the input file and location of the model to load.
#-------------------------------------------------------------------------------------------------------------------------------------  
def runSavedML(xfilename):
# Get the user to select the file containing the saved model - a separate Explorer window opens for selection
  ok = False
  while not ok:
    print("\nSelect the file containing the model to be used")
    filename = fd.askopenfilename(filetypes=[("sav Files", "*.sav")])
    actFile = os.path.dirname(filename) + "/activities.csv"   
    if not os.path.exists(actFile):
      print("\nThe file activities.csv must exist in the same folder as the model. Please select another file or recreate the model using this code.")
    else:
      ok = True    

# Show the user what activities were contained in the data that was used to create the model.  
  aList, TIME_STEPS , STEP = showModelActivities(actFile, False)

  df, newVals, rmsVals, trimStart, trimEnd = getUnlabelledData(xfilename)
    
  X = df.drop("timestamp", axis=1)
  X = X.values
          
# load the model and get the activity predictions
  loaded_model = pickle.load(open(filename, 'rb'))
  predictedAct = loaded_model.predict(X)
 
# 'Smooth' the predictions - for each 2 second interval set all the activity values to the most common value for that period.
# The model might for example predict 1/10th of a second of clapping during a 10 second period of walking - it is assumed that activities
# are performed in minimum time blocks of at least 2 seconds.
  activities = smoothActivities(predictedAct)  
  uniqueAct = list(set(activities))

# Get the user to associate predicted numeric values with actual activities (by showing them a plot of original data and predictions)
  actNums = predictedActivitySummary(activities,uniqueAct,freq)
  
# Display a plot of the original data and the predicted activities  
  plotPredictedActivities(actNums,newVals)

# Create an output file containing the original data, plus root mean square values and predicted activities.  
  produceLabelledFile(xfilename,False,aList,uniqueAct,activities,trimEnd,trimStart,rmsVals,"ML01Jul2022")
  exit()

#-------------------------------------------------------------------------------------------------------------------------------------  
# Run a previously saved keras (neural network) model against an unlabelled file from the sensor, and create a file containing the
# original data plus the root mean square value and predicted activity for each input record, if requested by the user.
# The user selects the input file and location of the model to load.
#-------------------------------------------------------------------------------------------------------------------------------------    
def runSavedKeras(xfilename):
  tf.get_logger().setLevel('ERROR')

# Get the user to select the folder containing the saved model - a separate Explorer window opens for selection
  filename, actFile, directoryname = getModelFolder("keras","saved_model.pb")

# Show the user what activities were contained in the data that was used to create the model.  
  aList, TIME_STEPS , STEP = showModelActivities(actFile, True)

  df, newVals, rmsVals, trimStart, trimEnd = getUnlabelledData(xfilename)
  
  X_test = create_dataset2(df[['x_axis', 'y_axis', 'z_axis']],TIME_STEPS,1)

# load the model
  print("\nPlease wait. Loading model ...")
  new_model = tf.keras.models.load_model(directoryname)

# Get the activity predictions
  predict_x=new_model.predict(X_test)  
  predictedAct=np.argmax(predict_x,axis=1)
  
# 'Smooth' the predictions - for each 2 second interval set all the activity values to the most common value for that period.
# The model might for example predict 1/10th of a second of clapping during a 10 second period of walking - it is assumed that activities
# are performed in minimum time blocks of at least 2 seconds.
  activities = smoothActivities(predictedAct)
  
  uniqueAct = list(set(activities))
  
# Show the user a summary of predicted activities
  actNums = predictedActivitySummary(activities,uniqueAct,freq)

# Display a plot of the original data and the predicted activities    
  plotPredictedActivities(activities,newVals)

# Create an output file containing the original data, plus root mean square values and predicted activities.  
  produceLabelledFile(xfilename,True,aList,uniqueAct,activities,trimEnd,trimStart,rmsVals,"NN01Jul2022")
  
  exit()
  
#--------------------------------------------------------------------------------------------------------------------------------------------
# Create an output csv file with the original data, plus a root mean square value of the x, y and z values for each record, and an activity
# label provided by the user.
# The number of records saved per second can be reduced from the original value by randomly selecting a specified number per second.
# The user selects periods of activity from a plot of the data, then specifies which activity is associated with which time period.
# Also produce a summary file with the means and standard deviations for each variable for each block of activity - in time order and then
# grouped by activity type.
#--------------------------------------------------------------------------------------------------------------------------------------------
def labelRawCsv(filename):
  global options , ivals , startTime , stopTime , xvals, yvals, zvals, rms , summary, row , svm , ns , freq, newVals
  last_1 = get_datetime_format(filename)

# Show the user how many records exist in the original file per second (the frequency), and ask how many they wish to retain per second.
  print("\nRaw data has ",freq," readings per second")
  perSec = getPerSec(freq)
  tfList = setTFList(perSec,freq)

  half = int(perSec/2)+1

# Get the user to specify what activities were performed for which time periods by displaying a plot of the data from the sensor and getting
# them to left click all the transitions in the data between activities. They are then shown all the time periods they have defined and they 
# assign an activity to each one. 
  n = 100
  m = 4
  ivals = [[0] * m for i in range(n)]
  options = setupList(activityList) 
  print("\nDisplaying data file for activity selection ...")
  print("Note - if the magnifier is used click y-values well above/below clicks used for activity selection.")
  plot_raw_data(filename,False,'Select timestamps of transitions between activities using mouse left-click. Finish inputs with mouse right-click. Remove entry with delete/backspace.')

# Ask the user for the subject id for the data file (csv output file from the Axivity sensor) being processed eg the 
# initials of the person's name who carried out the activities.
  subjectId = getSubjectId()

# Ask the user where was the sensor location for the selected data file (eg left wrist) during the associated activities
# The subjectid and sensor location are incorporated into the output file name, eg if they entered 'AS' for the subjectid and Left Wrist
# for the sensor location, then the output filename would be the input file name plus 'ASLW' before the '.csv'.
  print("\nSelect sensor location : ")
  options = setupList(locationList) 
  location = letUserPick(options) 
  locval = location[-3:-1]

# Set up the output file name and summary file name (which is the output name plus '_sum' before the '.csv'    
  xfilename = str(os.path.splitext(filename)[0]) + locval + subjectId + ".csv"
  sfilename = str(os.path.splitext(filename)[0]) + locval + subjectId + "_sum.csv"

  outfile = open(xfilename, 'w', newline='')
  
  writer = csv.writer(outfile)

# Initialse stats variables
  iy, ix, cSec, ns = 0, 0, 0, 0
  xvals, yvals, zvals, rms, summary = [], [], [], [], []
  xMean, yMean, zMean, rMean, xstdev, ystdev, zstdev, rstdev = 0, 0, 0, 0, 0, 0, 0, 0

# Create a header line for the summary file
  summaryList = "Subject","Activity","Location","Time","Duration(s)","x mean","x std","y mean","y std","z mean","z std","rms mean","rms std"
  summary.append(summaryList)

# Loop through each record in the input file. Calculate stats (mean/standard deviation)for each variable for each separate period
# of activity. Randomly select records for each second, based on how many records the user opted to keep per second.
  tListx = []
  activities = []
  replaceTime = False 
  iTime = ""
  fTime = '%Y' + bchar + '%m' + bchar + '%d %H:%M:%S'
  
  with open(filename, 'r') as csvFile:
    reader = csv.reader(csvFile)
    for row in reader:  
      if replaceTime:
        saveTime = iTime      
      if sy:
        iTime = row[0][0:4] + bchar + row[0][5:7] + bchar + row[0][8:19] 
      else:
        iTime = row[0][6:10] + bchar + row[0][3:5] + bchar + row[0][0:2]+ row[0][10:19]
      if (iTime >= startTime):  
        if ivals[iy][1] == "Other":
          if not replaceTime:
            oldTime = datetime.datetime.strptime(iTime, fTime)
            newTime = oldTime + datetime.timedelta(seconds=1)
            replaceTime = True
          writeRow = False
        else:
          writeRow = True
          if replaceTime:
            if(iTime != saveTime):
              newTime = newTime + datetime.timedelta(seconds=1)
          else:
            newTime = datetime.datetime.strptime(iTime, fTime)
        ns+=1  
        if tfList[cSec]:  
          if (iTime > stopTime):
            if len(xvals) > 1:
              xMean, yMean, zMean, rMean, xstdev, ystdev, zstdev, rstdev, duration = getStats() 
              summaryList = subjectId,ivals[iy][1],location,iTime,duration,xMean,xstdev,yMean,ystdev,zMean,zstdev,rMean,rstdev  
              summary.append(summaryList)
            break
          svm = (float(row[1])**2 + float(row[2])**2 + float(row[3])**2)**0.5
          tList = [row[1],row[2],row[3]]
          tListx.append(tList)
          if (iTime <= ivals[iy][0]): 
            setVals()  
            if writeRow:   
              row[0] = newTime              
              row.extend([svm,ivals[iy][1]])  
              activities.append(ivals[iy][1])
              writer.writerow(row)
          else:
            ix+=1
            if ix < half:
              setVals()   
              if writeRow:     
                row[0] = newTime              
                row.extend([svm,ivals[iy][1]])
                activities.append(ivals[iy][1])
                writer.writerow(row)
            else: 
              ix=0
              if ivals[(iy+1)][1] != "Other":
                row[0] = newTime              
                row.extend([svm,ivals[(iy+1)][1]])  
                activities.append(ivals[(iy+1)][1])
                writer.writerow(row)                           
              xMean, yMean, zMean, rMean, xstdev, ystdev, zstdev, rstdev, duration = getStats() 
              xvals, yvals, zvals, rms = [], [], [], []
              summaryList = subjectId,ivals[iy][1],location,iTime,duration,xMean,xstdev,yMean,ystdev,zMean,zstdev,rMean,rstdev  
              summary.append(summaryList)
              setVals()
              iy+=1
              ns=0
      cSec+=1
      if cSec >= freq:
        cSec = 0
        
  outfile.close()
  csvFile.close()

# Write the summary records by time period into the summary file.
  outfile = open(sfilename, 'w', newline='')
  writer = csv.writer(outfile) 
  for x in summary:
    writer.writerow(x)

# Write a space line to the summary file, then sort the records by activity type and write them again to summary.
  writer.writerow(" ")
  
  del summary[0]  
  summary = sorted(summary, key=lambda x: x[1])  
  for x in summary:
    writer.writerow(x)
  
  print("Writing output file ",sfilename)  
  outfile.close()

# Display a plot of the output file with the specified activities if the user wishes.
  ok = ""
  while ok != "Y" and ok != "N":
    ok = input("Plot raw data for labelled csv (Y/N) : ").upper()
    if ok == "Y":
      actNums = []
      for ac in activities:
        actNums.append(uniqueAct.index(ac))
      if replaceTime:
        headers = ['Time', 'x_axis', 'y_axis', 'z_axis', 'rms', 'activity']
        df = pd.read_csv(xfilename,header=None,names=headers,engine='python',usecols=['Time', 'x_axis', 'y_axis', 'z_axis', 'rms'])
# create a list of dataframe values
        newVals = df.values.tolist()
      plotPredictedActivities(actNums,newVals)  

#---------------------------------------------------------------------------------------------------------------------------------
# Get the user to select the data file being processed. A separate file explorer window pops up from which they can
# click on the file, then on 'open'.
# Make a basic check on how many columns the file contains - according to the processing being performed a labelled or unlabelled 
# input file will be expected, with 6/4 columns respectively.
#----------------------------------------------------------------------------------------------------------------------------------
def getInputFile(purpose):
  filename = ""
  while filename == "":
    if "Combine" in purpose:
      print("\nSelect first input data file")    
    else:
      print("\nSelect input data file")
    filename = fd.askopenfilename(filetypes=[("csv Files", "*.csv")])
    print("\nFile selected - ",filename)

    freq, numcols = find_frequency(filename)

    if "Combine" in purpose:
      if numcols != 6:
        print("\nSelected file does not contain the correct data.")
        print("Six columns are expected - Timestamp, x-value, y-value, z-value, rms-vale and activity-type\n")
        filename = ""
    elif "from labelled" in purpose:
      if numcols != 6:
        print("\nSelected file does not contain the correct data.")
        print("Six columns are expected - Timestamp, x-value, y-value, z-value, rms-vale and activity-type\n")
        filename = ""
    else:       
      if numcols != 4:
        print("\nSelected file does not contain the correct data.")
        print("Four columns are expected - Timestamp, x-value, y-value, z-value\n")
        filename = ""
  return filename, freq

def createNewFile(xfilename):
    ok = False
    while not ok:
        newFile = input("\nEnter a new filename for use as the output file eg 'newcsv' (.csv will be appended to it) - ")
        if len(newFile) > 0:
          newFile = os.path.dirname(xfilename) + "/" + newFile + ".csv"
          isfile = os.path.isfile(newFile) 
          if isfile:
            ui = ""
            while ui != "C":
              ui = input("File already exists. Overwrite it (O), choose another name (C) or stop (S) : ").upper()    
              if ui == "S":
                print("\nTerminating ...")
                exit()
              if ui == "O":
                ok = True
                break
          else:                
            ok = True
  
# use copyfile()
    shutil.copyfile(xfilename,newFile)        
    return(newFile)

#-----------------------------------------------------------------------------------------------------------------------------------------------------------
# Combine multiple csv files that are labelled with activities into one output file.
# The first input file is simply copied to the output file and the timestamp of the last record in the file is saved.
# One second is added to this saved timestamp, and for the next input file all records are appended to the end of the output file, but with the
# timestamps replaced - starting with the (timestamp + 1 second) as the replacement value, and incrementing this by one second each time the 'second' value
# changes in the input file. For each subsequent file keep using the modified timestamp, adding one second for the start of each file.
#-----------------------------------------------------------------------------------------------------------------------------------------------------------
def combineFiles(xfilename):
# Create the output file (filename specified by the user) for combining all the input files into. Copy the first input file to the output file.
  newFile = createNewFile(xfilename)

# Get the last record from the first input file. 
  last_1 = get_datetime_format(xfilename)

# Add 1 second to the timestamp of the last record  
  fTime = '%Y' + bchar + '%m' + bchar + '%d %H:%M:%S' 
  oldTime = datetime.strptime(last_1,fTime)
  newTime = oldTime + timedelta(seconds=1)

# For each subsequent input file append all the records to the end of the output file, replacing the timestamp with one that begins 1 second after the end of the previous
# input file, and incrementing it by one second every time the 'second' value changes in the input timestamp.
  fnames = []
  filename = "x"
# The user keeps selecting input files until finished - at which point they select 'Cancel' in the Explorer window rather than 'Open'.  
  while filename != "":
    print("\nSelect next input data file ('Cancel' when finished entering files)")
    filename = fd.askopenfilename(filetypes=[("csv Files", "*.csv")])
# Make a basic check on the selected file - that it contains the correct number of columns    
    if filename != "":
      print("\nFile selected - ",filename)
      freq, numcols = find_frequency(filename)
      if numcols == 6:
        fnames.append(filename)
      else:
        print("\nSelected file does not contain the correct data.")
        print("Six columns are expected - Timestamp, x-value, y-value, z-value, rms-vale and activity-type\n")
        filename = "x"

# Open the output file for appending  
  with open(newFile, 'a', newline='') as newf: 
    write = csv.writer(newf) 
# Process each input file in turn, reading all the records from them and replacing the timestamps before writing to the output file.    
    for fname in fnames:
      with open(fname, 'r') as csvFile:
        reader = csv.reader(csvFile)
        firstRec = True
        for row in reader:    
          if firstRec:
            saveTime = row[0]
            firstRec = False
          else:
# When the 'second' value changes in the input file, increment the timestamp used by one second          
            if row[0] != saveTime:
              saveTime = row[0]
              newTime = newTime + timedelta(seconds=1)
# Write a record to the output file using a newly calculated timestamp plus all the other values from the input file (x, y, z, rms, activity)              
          write.writerow([newTime,row[1],row[2],row[3],row[4],row[5]])
      csvFile.close()
      newTime = newTime + timedelta(seconds=1)
  newf.close()  
  exit()


#-------------------
# End of functions |
#-------------------

#------------------
# Main processing |
#------------------

# Initialise two variables used in formatting timestamps
sy , bchar = False , " "

# Ask the user what processing they are performing 
print("\nSelect required processing : ")
options = setupList(purposeList) 
purpose = letUserPick(options) 

# If 'help' is selected, output help text to the console or a file, 'helptext.txt', according to user preference.
if "Help" in purpose: 
  printTarget = ""
  while printTarget != "C" and printTarget != "F":
    printTarget = input("\nOutput Help to Console (C) or file 'helptext.txt' in current folder (F) - ").upper()

  if printTarget == "F":
    import sys
    with open('helptext.txt', 'w') as f:
      sys.stdout = f 
      print(helpText) 
  else:
    print(helpText) 
  exit()  

loadLibraries()

# Get the user to select the data file being processed. A separate file explorer window pops up from which they can
# click on the file, then on 'open'
filename, freq = getInputFile(purpose)

# Perform the required processing, as requested by the user
if "Run Neural" in purpose:
  runSavedKeras(filename)
elif "Run kMeans" in purpose:
  runSavedKmeans(filename)
elif "Run Machine" in purpose:
  runSavedML(filename)
elif "kMeans" in purpose:  
  kmeansAnalysis(filename)
elif "Machine Learning" in purpose:  
  machineLearnAnalysis(filename) 
elif "Create Neural" in purpose:  
  kerasAnalysis (filename)  
elif "Combine" in purpose:  
  combineFiles(filename) 
else:
  labelRawCsv(filename)

#--------------
# End of code |
#--------------


